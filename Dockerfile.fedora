# syntax=docker/dockerfile:1.4
FROM fedora:43

# ====== Version Configuration (from versions.env) ======
ARG ROCM_VERSION="6.3"
ARG TORCH_VERSION="2.5.0+rocm6.3"
ARG PYTHON_VERSION="3.12"
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"
ARG VLLM_VERSION="v0.10.2"  # Latest stable release
ARG FLASH_ATTENTION_REPO="https://github.com/ROCm/flash-attention.git"
ARG FLASH_ATTENTION_COMMIT="main_perf"

LABEL maintainer="Antigravity Build System"
LABEL description="vLLM + PyTorch on ROCm gfx1151 (Strix Point/Halo) - Fedora/RHEL Build"
LABEL vllm.version="${VLLM_VERSION}"
LABEL torch.version="${TORCH_VERSION}"
LABEL rocm.version="${ROCM_VERSION}"

# ====== Base System Setup ======
RUN dnf install -y \
    wget curl git \
    gcc gcc-c++ make cmake ninja-build \
    python3-pip python3-devel \
    openssl-devel libffi-devel \
    ca-certificates tar gzip libatomic \
    perl numactl-libs numactl-devel \
    && dnf clean all

# ====== Install ROCm 6.3 (From Official AMD RPMs for RHEL9) ======
# Note: Using RHEL9 packages on Fedora 43
RUN echo -e "[ROCm-${ROCM_VERSION}]\nname=ROCm${ROCM_VERSION}\nbaseurl=https://repo.radeon.com/rocm/rhel9/${ROCM_VERSION}/main\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.radeon.com/rocm/rocm.gpg.key" \
    | tee /etc/yum.repos.d/rocm.repo && \
    dnf install -y \
    rocm-dev rocm-libs rocm-hip-sdk rocm-smi-lib \
    && dnf clean all

ENV ROCM_PATH=/opt/rocm
ENV LD_LIBRARY_PATH=$ROCM_PATH/lib:$LD_LIBRARY_PATH
ENV PATH=$ROCM_PATH/bin:$PATH
ENV DEVICE_LIB_PATH=$ROCM_PATH/amdgcn/bitcode
ENV HIP_DEVICE_LIB_PATH=$ROCM_PATH/amdgcn/bitcode
ENV PYTORCH_ROCM_ARCH="gfx1151"
ENV HSA_OVERRIDE_GFX_VERSION=11.5.1

# ====== Install uv for Python package management ======
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /opt/vllm-build
RUN uv venv --python ${PYTHON_VERSION}
ENV VIRTUAL_ENV=/opt/vllm-build/.venv
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# ====== Install PyTorch 2.5.0 (Stable) ======
# Note: Using latest from ROCm 6.3 index (version constraint doesn't work with index-url)
RUN uv pip install \
    --index-url https://download.pytorch.org/whl/rocm6.3 \
    torch \
    torchvision \
    torchaudio

# Verify torch installation
RUN python -c "import torch; print(f'PyTorch: {torch.__version__}, ROCm: {torch.version.hip}')"

# ====== Clone vLLM v0.10.3 (Release Tag) ======
RUN git clone "${VLLM_REPO}" vllm && \
    cd vllm && \
    git fetch --all --tags && \
    git checkout "${VLLM_VERSION}" && \
    # Patch: Add gfx1151 to supported architectures (robust patch)
    sed -i 's/set(HIP_SUPPORTED_ARCHS "/set(HIP_SUPPORTED_ARCHS "gfx1151;/g' CMakeLists.txt && \
    # Patch: Fallback to torch.version.hip if amdsmi fails
    sed -i 's/return "vllm.platforms.rocm.RocmPlatform" if is_rocm else None/import torch; return "vllm.platforms.rocm.RocmPlatform" if (is_rocm or torch.version.hip) else None/g' vllm/platforms/__init__.py

WORKDIR /opt/vllm-build/vllm

# ====== Build vLLM ======
ENV VLLM_TARGET_DEVICE="rocm"
ENV MAX_JOBS="4"
ENV FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE"

# Install vLLM dependencies
RUN uv pip install -r requirements/rocm.txt && \
    uv pip install \
    setuptools setuptools_scm wheel ninja \
    cmake scikit-build-core

# Build and install vLLM
RUN CMAKE_PREFIX_PATH="${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch/share/cmake/Torch" \
    TORCH_CUDA_ARCH_LIST="gfx1151" \
    uv pip install --no-build-isolation -e .

# ====== Optional: Build Flash Attention ======
RUN cd /opt/vllm-build && \
    git clone "${FLASH_ATTENTION_REPO}" flash-attention && \
    cd flash-attention && \
    git checkout "${FLASH_ATTENTION_COMMIT}" && \
    MAX_JOBS=4 FLASH_ATTENTION_TRITON_AMD_ENABLE=TRUE \
    python setup.py install

# ====== Verification & Runtime Setup ======
RUN python -c "import vllm; print(f'vLLM: {vllm.__version__}')" && \
    python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Devices: {torch.cuda.device_count()}')"

WORKDIR /workspace
EXPOSE 8000

CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
    "--host", "0.0.0.0", "--port", "8000", \
    "--model", "cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit", \
    "--dtype", "float16", \
    "--max-model-len", "32768"]
