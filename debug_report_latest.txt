=== DATE ===
Sun Dec  7 18:51:47 EST 2025

=== SYSTEMCTL STATUS ===
â— nvidia-nim@qwen32b.service - NVIDIA NIM Inference Service - qwen32b
     Loaded: loaded (/etc/systemd/system/nvidia-nim@.service; enabled; preset: enabled)
     Active: active (running) since Sun 2025-12-07 18:46:44 EST; 5min ago
    Process: 4941 ExecStartPre=/usr/bin/docker stop nim-qwen32b (code=exited, status=1/FAILURE)
    Process: 4953 ExecStartPre=/usr/bin/docker rm nim-qwen32b (code=exited, status=1/FAILURE)
   Main PID: 4965 (docker)
      Tasks: 13 (limit: 153568)
     Memory: 9.9M (peak: 11.4M)
        CPU: 64ms
     CGroup: /system.slice/system-nvidia\x2dnim.slice/nvidia-nim@qwen32b.service
             â””â”€4965 /usr/bin/docker run --rm --name nim-qwen32b --gpus all --shm-size=16g -e ACCEPT_NVIDIA_TOS=1 -e NGC_API_KEY=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 -e NIM_GPU_MEM_FRACTION=0.50 -v /home/jowest/.cache/nim:/opt/nim/.cache -v /etc/nvidia-nim:/etc/nvidia-nim:ro -p 8001:8000 nvcr.io/nim/qwen/qwen3-32b-dgx-spark:1.1.0-variant

Dec 07 18:50:27 jowestdgx bash[4965]:     "top_p": 1,
Dec 07 18:50:27 jowestdgx bash[4965]:     "n": 1,
Dec 07 18:50:27 jowestdgx bash[4965]:     "max_tokens": 15,
Dec 07 18:50:27 jowestdgx bash[4965]:     "frequency_penalty": 1.0,
Dec 07 18:50:27 jowestdgx bash[4965]:     "stop": ["hello"]
Dec 07 18:50:27 jowestdgx bash[4965]:   }'
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 on.py:62] Application startup complete.
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
Dec 07 18:50:41 jowestdgx bash[4965]: INFO 2025-12-07 23:50:41.017 httptools_impl.py:481] 192.168.88.247:63149 - "GET /v1/models HTTP/1.1" 200
Dec 07 18:50:41 jowestdgx bash[4965]: INFO 2025-12-07 23:50:41.556 httptools_impl.py:481] 192.168.88.247:63149 - "GET /v1/models HTTP/1.1" 200

â— nvidia-nim@nemotron9b.service - NVIDIA NIM Inference Service - nemotron9b
     Loaded: loaded (/etc/systemd/system/nvidia-nim@.service; disabled; preset: enabled)
     Active: active (running) since Sun 2025-12-07 18:47:56 EST; 3min 50s ago
    Process: 5497 ExecStartPre=/usr/bin/docker stop nim-nemotron9b (code=exited, status=1/FAILURE)
    Process: 5508 ExecStartPre=/usr/bin/docker rm nim-nemotron9b (code=exited, status=1/FAILURE)
   Main PID: 5518 (docker)
      Tasks: 13 (limit: 153568)
     Memory: 9.5M (peak: 11.4M)
        CPU: 58ms
     CGroup: /system.slice/system-nvidia\x2dnim.slice/nvidia-nim@nemotron9b.service
             â””â”€5518 /usr/bin/docker run --rm --name nim-nemotron9b --gpus all --shm-size=16g -e ACCEPT_NVIDIA_TOS=1 -e NGC_API_KEY=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 -e NIM_GPU_MEM_FRACTION=0.15 -v /home/jowest/.cache/nim:/opt/nim/.cache -v /etc/nvidia-nim:/etc/nvidia-nim:ro -p 8003:8000 nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2-dgx-spark:1.0.0-variant

Dec 07 18:50:13 jowestdgx bash[5518]:     with launch_core_engines(vllm_config, executor_class,
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
Dec 07 18:50:13 jowestdgx bash[5518]:     next(self.gen)
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
Dec 07 18:50:13 jowestdgx bash[5518]:     wait_for_engine_startup(
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
Dec 07 18:50:13 jowestdgx bash[5518]:     raise RuntimeError("Engine core initialization failed. "
Dec 07 18:50:13 jowestdgx bash[5518]: RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Dec 07 18:50:13 jowestdgx bash[5518]: ERROR 2025-12-07 23:50:13.869 on.py:59] Application startup failed. Exiting.
Dec 07 18:50:13 jowestdgx bash[5518]: INFO 2025-12-07 23:50:13.869 http_api.py:171] HTTP Inference server has exited.

â— nvidia-nim@llama8b.service - NVIDIA NIM Inference Service - llama8b
     Loaded: loaded (/etc/systemd/system/nvidia-nim@.service; disabled; preset: enabled)
     Active: active (running) since Sun 2025-12-07 18:48:03 EST; 3min 43s ago
    Process: 5791 ExecStartPre=/usr/bin/docker stop nim-llama8b (code=exited, status=1/FAILURE)
    Process: 5803 ExecStartPre=/usr/bin/docker rm nim-llama8b (code=exited, status=1/FAILURE)
   Main PID: 5816 (docker)
      Tasks: 12 (limit: 153568)
     Memory: 9.4M (peak: 11.1M)
        CPU: 61ms
     CGroup: /system.slice/system-nvidia\x2dnim.slice/nvidia-nim@llama8b.service
             â””â”€5816 /usr/bin/docker run --rm --name nim-llama8b --gpus all --shm-size=16g -e ACCEPT_NVIDIA_TOS=1 -e NGC_API_KEY=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 -e NIM_GPU_MEM_FRACTION=0.15 -v /home/jowest/.cache/nim:/opt/nim/.cache -v /etc/nvidia-nim:/etc/nvidia-nim:ro -p 8002:8000 nvcr.io/nim/meta/llama-3.1-8b-instruct-dgx-spark:1.0.0-variant

Dec 07 18:49:52 jowestdgx bash[5816]:     with launch_core_engines(vllm_config, executor_class,
Dec 07 18:49:52 jowestdgx bash[5816]:   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
Dec 07 18:49:52 jowestdgx bash[5816]:     next(self.gen)
Dec 07 18:49:52 jowestdgx bash[5816]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 706, in launch_core_engines
Dec 07 18:49:52 jowestdgx bash[5816]:     wait_for_engine_startup(
Dec 07 18:49:52 jowestdgx bash[5816]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
Dec 07 18:49:52 jowestdgx bash[5816]:     raise RuntimeError("Engine core initialization failed. "
Dec 07 18:49:52 jowestdgx bash[5816]: RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Dec 07 18:49:52 jowestdgx bash[5816]: ERROR 2025-12-07 23:49:52.819 on.py:59] Application startup failed. Exiting.
Dec 07 18:49:52 jowestdgx bash[5816]: INFO 2025-12-07 23:49:52.820 http_api.py:170] HTTP Inference server has exited.

=== DOCKER CONTAINERS ===
CONTAINER ID   IMAGE                                                                   COMMAND                  CREATED         STATUS         PORTS                                                             NAMES
da0c59bcc140   nvcr.io/nim/meta/llama-3.1-8b-instruct-dgx-spark:1.0.0-variant          "/opt/nvidia/nvidia_â€¦"   3 minutes ago   Up 3 minutes   6006/tcp, 8888/tcp, 0.0.0.0:8002->8000/tcp, [::]:8002->8000/tcp   nim-llama8b
6294244d26f4   nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2-dgx-spark:1.0.0-variant   "/opt/nvidia/nvidia_â€¦"   3 minutes ago   Up 3 minutes   6006/tcp, 8888/tcp, 0.0.0.0:8003->8000/tcp, [::]:8003->8000/tcp   nim-nemotron9b
cb4abe99e553   nvcr.io/nim/qwen/qwen3-32b-dgx-spark:1.1.0-variant                      "/opt/nvidia/nvidia_â€¦"   5 minutes ago   Up 5 minutes   6006/tcp, 8888/tcp, 0.0.0.0:8001->8000/tcp, [::]:8001->8000/tcp   nim-qwen32b

=== DOCKER LOGS (nim-nemotron9b) ===

===========================================
== NVIDIA Inference Microservice LLM NIM ==
===========================================
NVIDIA Inference Microservice LLM NIM GA 1.0.0
Model: nvidia/nemotron-nano-9b-v2

Container image Copyright (c) 2016-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

The NIM container is governed by the NVIDIA Software License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and the Product-Specific Terms for NVIDIA AI Products (found at https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/); and the use of this model is governed by the NVIDIA Open Model License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/).

INFO 2025-12-07 23:47:57.887 __init__.py:457] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2025-12-07 23:47:57.888 check_cache_env.py:46] /opt/nim/.cache is present and is writable.

INFO 2025-12-07 23:47:58.006 __init__.py:457] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2025-12-07 23:47:58.691 start_server.py:58] Starting nimlib 0.11.12 nim_sdk 0.9.6
INFO 2025-12-07 23:47:58.691 standard_files.py:95] NIM VERSION:
1.0.0

INFO 12-07 23:48:03 [__init__.py:216] Automatically detected platform cuda.
INFO 2025-12-07 23:48:04.103 inference.py:72] Creating inference config
INFO 2025-12-07 23:48:04.104 profiles.py:101] Registered custom profile selectors: []
SYSTEM INFO
- Free GPUs:
  -  [2e12:10de] (0) NVIDIA GB10 [current utilization: N/A%]
INFO 2025-12-07 23:48:04.121 profile_report.py:268] NIM_TAGS_SELECTOR environment variable is not set.
SYSTEM INFO
- Free GPUs:
  -  [2e12:10de] (0) NVIDIA GB10 [current utilization: N/A%]
INFO 2025-12-07 23:48:04.121 profile_report.py:268] NIM_TAGS_SELECTOR environment variable is not set.
WARNING 2025-12-07 23:48:04.122 profile_memory.py:102] No model_family in profile f9db61d5a467d46be7733e90a26705798764b93d480c842143d525c1932b01e2
INFO 2025-12-07 23:48:04.122 profiles.py:341] Matched profile_id in manifest from LLMBasedProfileSelector f9db61d5a467d46be7733e90a26705798764b93d480c842143d525c1932b01e2 with tags: {'feat_lora': 'false', 'llm_engine': 'vllm', 'nim_workspace_hash_v1': '1bc1fc30fed1c38789e3f288e06ed98a1c855a9c23ada6c501d0f732336b0121', 'pp': '1', 'precision': 'nvfp4', 'tp': '1'}
INFO 2025-12-07 23:48:04.122 nim_sdk.py:313] Using the profile selected by the profile selector: f9db61d5a467d46be7733e90a26705798764b93d480c842143d525c1932b01e2
INFO 2025-12-07 23:48:04.123 nim_sdk.py:326] Downloading manifest profile: f9db61d5a467d46be7733e90a26705798764b93d480c842143d525c1932b01e2
INFO 2025-12-07 23:48:04.157 lib.rs:203] File: checksums.blake3 found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/checksums.blake3"
INFO 2025-12-07 23:48:04.157 public.rs:52] Skipping download, using cached copy of file: checksums.blake3 at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/checksums.blake3"
INFO 2025-12-07 23:48:04.170 lib.rs:203] File: model-00007-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00007-of-00010.safetensors"
INFO 2025-12-07 23:48:04.170 public.rs:52] Skipping download, using cached copy of file: model-00007-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00007-of-00010.safetensors"
INFO 2025-12-07 23:48:04.182 lib.rs:203] File: model-00006-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00006-of-00010.json"
INFO 2025-12-07 23:48:04.182 public.rs:52] Skipping download, using cached copy of file: model-00006-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00006-of-00010.json"
INFO 2025-12-07 23:48:04.194 lib.rs:203] File: model-00003-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00003-of-00010.safetensors"
INFO 2025-12-07 23:48:04.194 public.rs:52] Skipping download, using cached copy of file: model-00003-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00003-of-00010.safetensors"
INFO 2025-12-07 23:48:04.207 lib.rs:203] File: model-00006-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00006-of-00010.safetensors"
INFO 2025-12-07 23:48:04.207 public.rs:52] Skipping download, using cached copy of file: model-00006-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00006-of-00010.safetensors"
INFO 2025-12-07 23:48:04.213 lib.rs:203] File: model-00003-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00003-of-00010.json"
INFO 2025-12-07 23:48:04.213 public.rs:52] Skipping download, using cached copy of file: model-00003-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00003-of-00010.json"
INFO 2025-12-07 23:48:04.219 lib.rs:203] File: model-00007-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00007-of-00010.json"
INFO 2025-12-07 23:48:04.219 public.rs:52] Skipping download, using cached copy of file: model-00007-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00007-of-00010.json"
INFO 2025-12-07 23:48:04.225 lib.rs:203] File: model-00010-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00010-of-00010.safetensors"
INFO 2025-12-07 23:48:04.225 public.rs:52] Skipping download, using cached copy of file: model-00010-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00010-of-00010.safetensors"
INFO 2025-12-07 23:48:04.231 lib.rs:203] File: model-00004-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00004-of-00010.json"
INFO 2025-12-07 23:48:04.231 public.rs:52] Skipping download, using cached copy of file: model-00004-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00004-of-00010.json"
INFO 2025-12-07 23:48:04.237 lib.rs:203] File: model-00005-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00005-of-00010.json"
INFO 2025-12-07 23:48:04.237 public.rs:52] Skipping download, using cached copy of file: model-00005-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00005-of-00010.json"
INFO 2025-12-07 23:48:04.243 lib.rs:203] File: model-00009-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00009-of-00010.safetensors"
INFO 2025-12-07 23:48:04.243 public.rs:52] Skipping download, using cached copy of file: model-00009-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00009-of-00010.safetensors"
INFO 2025-12-07 23:48:04.249 lib.rs:203] File: special_tokens_map.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/special_tokens_map.json"
INFO 2025-12-07 23:48:04.249 public.rs:52] Skipping download, using cached copy of file: special_tokens_map.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/special_tokens_map.json"
INFO 2025-12-07 23:48:04.255 lib.rs:203] File: model-00002-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00002-of-00010.safetensors"
INFO 2025-12-07 23:48:04.255 public.rs:52] Skipping download, using cached copy of file: model-00002-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00002-of-00010.safetensors"
INFO 2025-12-07 23:48:04.261 lib.rs:203] File: model.safetensors.index.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model.safetensors.index.json"
INFO 2025-12-07 23:48:04.261 public.rs:52] Skipping download, using cached copy of file: model.safetensors.index.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model.safetensors.index.json"
INFO 2025-12-07 23:48:04.267 lib.rs:203] File: model-00005-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00005-of-00010.safetensors"
INFO 2025-12-07 23:48:04.267 public.rs:52] Skipping download, using cached copy of file: model-00005-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00005-of-00010.safetensors"
INFO 2025-12-07 23:48:04.273 lib.rs:203] File: config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/config.json"
INFO 2025-12-07 23:48:04.273 public.rs:52] Skipping download, using cached copy of file: config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/config.json"
INFO 2025-12-07 23:48:04.279 lib.rs:203] File: model-00008-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00008-of-00010.json"
INFO 2025-12-07 23:48:04.279 public.rs:52] Skipping download, using cached copy of file: model-00008-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00008-of-00010.json"
INFO 2025-12-07 23:48:04.285 lib.rs:203] File: model-00010-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00010-of-00010.json"
INFO 2025-12-07 23:48:04.285 public.rs:52] Skipping download, using cached copy of file: model-00010-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00010-of-00010.json"
INFO 2025-12-07 23:48:04.291 lib.rs:203] File: model-00001-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00001-of-00010.safetensors"
INFO 2025-12-07 23:48:04.291 public.rs:52] Skipping download, using cached copy of file: model-00001-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00001-of-00010.safetensors"
INFO 2025-12-07 23:48:04.297 lib.rs:203] File: model-00008-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00008-of-00010.safetensors"
INFO 2025-12-07 23:48:04.297 public.rs:52] Skipping download, using cached copy of file: model-00008-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00008-of-00010.safetensors"
INFO 2025-12-07 23:48:04.303 lib.rs:203] File: model-00004-of-00010.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00004-of-00010.safetensors"
INFO 2025-12-07 23:48:04.303 public.rs:52] Skipping download, using cached copy of file: model-00004-of-00010.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00004-of-00010.safetensors"
INFO 2025-12-07 23:48:04.309 lib.rs:203] File: hf_quant_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/hf_quant_config.json"
INFO 2025-12-07 23:48:04.309 public.rs:52] Skipping download, using cached copy of file: hf_quant_config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/hf_quant_config.json"
INFO 2025-12-07 23:48:04.315 lib.rs:203] File: model-00009-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00009-of-00010.json"
INFO 2025-12-07 23:48:04.315 public.rs:52] Skipping download, using cached copy of file: model-00009-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00009-of-00010.json"
INFO 2025-12-07 23:48:04.321 lib.rs:203] File: tokenizer.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/tokenizer.json"
INFO 2025-12-07 23:48:04.321 public.rs:52] Skipping download, using cached copy of file: tokenizer.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/tokenizer.json"
INFO 2025-12-07 23:48:04.327 lib.rs:203] File: tokenizer_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/tokenizer_config.json"
INFO 2025-12-07 23:48:04.327 public.rs:52] Skipping download, using cached copy of file: tokenizer_config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/tokenizer_config.json"
INFO 2025-12-07 23:48:04.333 lib.rs:203] File: model-00001-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00001-of-00010.json"
INFO 2025-12-07 23:48:04.333 public.rs:52] Skipping download, using cached copy of file: model-00001-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00001-of-00010.json"
INFO 2025-12-07 23:48:04.339 lib.rs:203] File: generation_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/generation_config.json"
INFO 2025-12-07 23:48:04.339 public.rs:52] Skipping download, using cached copy of file: generation_config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/generation_config.json"
INFO 2025-12-07 23:48:04.345 lib.rs:203] File: model-00002-of-00010.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00002-of-00010.json"
INFO 2025-12-07 23:48:04.345 public.rs:52] Skipping download, using cached copy of file: model-00002-of-00010.json at path: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--nemotron-nano-9b-v2/snapshots/hf-nvfp4-v1/model-00002-of-00010.json"
INFO 2025-12-07 23:48:04.355 nim_sdk.py:346] Using the workspace specified during init: /opt/nim/workspace
INFO 2025-12-07 23:48:04.355 nim_sdk.py:352] Creating workspace at /opt/nim/workspace
INFO 2025-12-07 23:48:04.356 nim_sdk.py:359] Materializing workspace to: /opt/nim/workspace
INFO 2025-12-07 23:48:05.292 nimutils.py:259] Starting NIM inference server
INFO 2025-12-07 23:48:05.292 inference.py:445] Starting application
WARN 2025-12-07 23:48:05.293 system.rs:194] Not supported error accessing attributes of Device with index: 0 error: the requested operation is not available on the target device
INFO 2025-12-07 23:48:05.299 telemetry_handler.py:80] Telemetry mode TelemetryMode.OFF
INFO 2025-12-07 23:48:05.346 inference.py:143] Interface initialized
INFO 2025-12-07 23:48:05.346 inference.py:450] Starting server
INFO 2025-12-07 23:48:05.346 http_api.py:111] Serving endpoints:
  0.0.0.0:8000/v1/models (GET)
  0.0.0.0:8000/v1/chat/completions (POST)
  0.0.0.0:8000/v1/completions (POST)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/version (GET)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/metrics (GET)
  0.0.0.0:8000/v1/license (GET)
  0.0.0.0:8000/v1/metadata (GET)
  0.0.0.0:8000/v1/manifest (GET)
INFO 2025-12-07 23:48:05.347 http_api.py:135] {'message': 'Starting HTTP Inference server', 'port': 8000, 'workers_count': 1, 'host': '0.0.0.0', 'log_level': 'info', 'SSL': 'disabled'}
INFO 2025-12-07 23:48:05.404 server.py:82] Started server process [83]
INFO 2025-12-07 23:48:05.405 on.py:48] Waiting for application startup.
INFO 2025-12-07 23:48:05.405 inference.py:149] Starting engine initialization with model: nvidia/nemotron-nano-9b-v2
INFO 2025-12-07 23:48:05.510 vllm_backend.py:70] Initializing tokenizer
INFO 2025-12-07 23:48:05.510 vllm_backend.py:77] Using tokenizer source: /opt/nim/workspace
INFO 2025-12-07 23:48:05.857 vllm_backend.py:83] Tokenizer initialized successfully
INFO 2025-12-07 23:48:05.857 vllm_backend.py:105] Using model source for engine: /opt/nim/workspace
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-07 23:48:10 [__init__.py:742] Resolved architecture: NemotronHForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 12-07 23:48:10 [__init__.py:1815] Using max model len 131072
WARNING 12-07 23:48:11 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'
INFO 12-07 23:48:11 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 12-07 23:48:11 [config.py:310] Hybrid or mamba-based model detected: disabling prefix caching since it is not yet supported.
INFO 12-07 23:48:11 [config.py:321] Hybrid or mamba-based model detected: setting cudagraph mode to FULL_AND_PIECEWISE in order to optimize performance.
INFO 12-07 23:48:11 [config.py:390] Setting attention block size to 1312 tokens to ensure that attention page size is >= mamba page size.
INFO 12-07 23:48:11 [config.py:411] Padding mamba page size by 1.08% to ensure that mamba page size and attention page size are exactly equal.
WARNING 12-07 23:48:11 [modelopt.py:606] Detected ModelOpt NVFP4 checkpoint. Please note that the format is experimental and could change in future.
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:48:11 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:48:11 [core.py:76] Initializing a V1 LLM engine (v0.10.2+d8fcb151.nvmain) with config: model='/opt/nim/workspace', speculative_config=None, tokenizer='/opt/nim/workspace', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt_fp4, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/nim/workspace, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":8,"local_cache_dir":null}
[W1207 23:48:13.774279833 ProcessGroupNCCL.cpp:936] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:48:13 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:48:13 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:48:13 [gpu_model_runner.py:2338] Starting to load model /opt/nim/workspace...
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:48:13 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:48:13 [cuda.py:322] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=198)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=198)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:22<00:22, 22.95s/it]
[1;36m(EngineCore_DP0 pid=198)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:46<00:00, 23.50s/it]
[1;36m(EngineCore_DP0 pid=198)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:46<00:00, 23.42s/it]
[1;36m(EngineCore_DP0 pid=198)[0;0m 
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:49:00 [default_loader.py:268] Loading weights took 47.04 seconds
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:49:01 [gpu_model_runner.py:2392] Model loading took 7.3380 GiB and 47.533178 seconds
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:49:03 [backends.py:539] Using cache directory: /tmp/vllm/cache/torch_compile_cache/2ba4f62aae/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:49:03 [backends.py:550] Dynamo bytecode transform time: 2.14 s
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:49:04 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=198)[0;0m [rank0]:W1207 23:49:04.644000 198 torch/_inductor/utils.py:1554] [0/0] Not enough SMs to use max_autotune_gemm mode
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:49:08 [backends.py:215] Compiling a graph for dynamic shape takes 4.29 s
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:49:42 [monitor.py:34] torch.compile takes 6.43 s in total
[1;36m(EngineCore_DP0 pid=198)[0;0m INFO 12-07 23:50:13 [gpu_worker.py:298] Available KV cache memory: 1.69 GiB
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 193, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1110, in get_kv_cache_config
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 708, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718]     raise ValueError(
[1;36m(EngineCore_DP0 pid=198)[0;0m ERROR 12-07 23:50:13 [core.py:718] ValueError: To serve at least one request with the models's max seq len (131072), (2.14 GiB KV cache is needed, which is larger than the available KV cache memory (1.69 GiB). Based on the available memory, the estimated maximum model length is 101024. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[1;36m(EngineCore_DP0 pid=198)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=198)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=198)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=198)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 722, in run_engine_core
[1;36m(EngineCore_DP0 pid=198)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=198)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=198)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=198)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=198)[0;0m     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 193, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=198)[0;0m     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1110, in get_kv_cache_config
[1;36m(EngineCore_DP0 pid=198)[0;0m     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_DP0 pid=198)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 708, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=198)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=198)[0;0m ValueError: To serve at least one request with the models's max seq len (131072), (2.14 GiB KV cache is needed, which is larger than the available KV cache memory (1.69 GiB). Based on the available memory, the estimated maximum model length is 101024. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
ERROR 2025-12-07 23:50:13.864 vllm_backend.py:242] Initialization error: Engine core initialization failed. See root cause above. Failed core proc(s): {}
ERROR 2025-12-07 23:50:13.865 vllm_backend.py:243] Error type: <class 'RuntimeError'>
ERROR 2025-12-07 23:50:13.867 vllm_backend.py:246] Full traceback: Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/backends/vllm_backend.py", line 144, in initialize
    self.engine = AsyncLLMEngine.from_engine_args(engine_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py", line 574, in from_engine_args
    return async_engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 1589, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 212, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 136, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 769, in __init__
    super().__init__(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 448, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR 2025-12-07 23:50:13.867 inference.py:165] Error during engine lifecycle: Engine core initialization failed. See root cause above. Failed core proc(s): {}
ERROR 2025-12-07 23:50:13.868 on.py:121] Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/starlette/routing.py", line 694, in lifespan
    async with self.lifespan_context(app) as maybe_state:
  File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/nimlib/nim_inference_api_builder/api.py", line 100, in wrapper
    async with lifespan_func():
  File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/nim/inference.py", line 151, in lifespan
    await self.engine.initialize()  # Properly await the async initialization
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/inference_engine.py", line 87, in initialize
    await self.backend.initialize()
  File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/backends/vllm_backend.py", line 144, in initialize
    self.engine = AsyncLLMEngine.from_engine_args(engine_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py", line 574, in from_engine_args
    return async_engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 1589, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 212, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 136, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 769, in __init__
    super().__init__(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 448, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR 2025-12-07 23:50:13.869 on.py:59] Application startup failed. Exiting.
INFO 2025-12-07 23:50:13.869 http_api.py:171] HTTP Inference server has exited.

=== DOCKER LOGS (nim-llama8b) ===

===========================================
== NVIDIA Inference Microservice LLM NIM ==
===========================================
NVIDIA Inference Microservice LLM NIM GA
Model: meta/llama-3.1-8b-instruct

Container image Copyright (c) 2016-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

INFO 2025-12-07 23:48:03.958 __init__.py:457] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2025-12-07 23:48:03.959 check_cache_env.py:46] /opt/nim/.cache is present and is writable.

INFO 2025-12-07 23:48:04.050 __init__.py:457] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
/usr/local/lib/python3.12/dist-packages/nimlib/hardware_inspect.py:26: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
INFO 2025-12-07 23:48:04.660 start_server.py:58] Starting nimlib 0.11.12 nim_sdk 0.9.6
INFO 2025-12-07 23:48:04.660 standard_files.py:95] NIM VERSION:
1.0.0
INFO 2025-12-07 23:48:04.661 standard_files.py:95] NIM banner.txt:
The NIM container is governed by the NVIDIA Software License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement) and the Product Specific Terms for AI Products (found at https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products).

A copy of this license can be found under /opt/nim/LICENSE.

The use of this model is governed by the NVIDIA Community Model License (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)

ADDITIONAL INFORMATION: Llama 3.1 Community License Agreement, Built with Llama.
INFO 2025-12-07 23:48:04.661 standard_files.py:95] NIM NOTICE:
Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
LicenseRef-NvidiaProprietary


NVIDIA CORPORATION, its affiliates and licensors retain all intellectual property and proprietary rights in and to this material, related documentation and any modifications thereto. Any use, reproduction, disclosure or distribution of this material and related documentation without an express license agreement from NVIDIA CORPORATION or its affiliates are strictly prohibited.


Llama 3.1 is licensed under the Llama 3.1 Community License, Copyright (c) Meta Platforms, Inc. All Rights Reserved.

INFO 12-07 23:48:08 [__init__.py:241] Automatically detected platform cuda.
INFO 2025-12-07 23:48:08.336 inference.py:72] Creating inference config
INFO 2025-12-07 23:48:08.336 profiles.py:105] Registered custom profile selectors: []
SYSTEM INFO
- Free GPUs:
  -  [2e12:10de] (0) NVIDIA GB10 [current utilization: N/A%]
INFO 2025-12-07 23:48:08.348 profile_report.py:268] NIM_TAGS_SELECTOR environment variable is not set.
SYSTEM INFO
- Free GPUs:
  -  [2e12:10de] (0) NVIDIA GB10 [current utilization: N/A%]
INFO 2025-12-07 23:48:08.348 profile_report.py:268] NIM_TAGS_SELECTOR environment variable is not set.
WARNING 2025-12-07 23:48:08.348 profile_memory.py:102] No model_family in profile bfb2c8a2d4dceba7d2629c856113eb12f5ce60d6de4e10369b4b334242229cfa
INFO 2025-12-07 23:48:08.348 profiles.py:345] Matched profile_id in manifest from LLMBasedProfileSelector bfb2c8a2d4dceba7d2629c856113eb12f5ce60d6de4e10369b4b334242229cfa with tags: {'feat_lora': 'false', 'gpu_device': '2e12:10de', 'llm_engine': 'vllm', 'nim_workspace_hash_v1': 'e8fd1d4848c7c3354b9a71c368608fd82e89e32a4b76493c25652b94722bce2d', 'pp': '1', 'precision': 'fp8', 'tp': '1'}
INFO 2025-12-07 23:48:08.348 nim_sdk.py:313] Using the profile selected by the profile selector: bfb2c8a2d4dceba7d2629c856113eb12f5ce60d6de4e10369b4b334242229cfa
INFO 2025-12-07 23:48:08.349 nim_sdk.py:326] Downloading manifest profile: bfb2c8a2d4dceba7d2629c856113eb12f5ce60d6de4e10369b4b334242229cfa
INFO 2025-12-07 23:48:08.372 lib.rs:203] File: hf_quant_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/hf_quant_config.json"
INFO 2025-12-07 23:48:08.372 public.rs:52] Skipping download, using cached copy of file: hf_quant_config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/hf_quant_config.json"
INFO 2025-12-07 23:48:08.379 lib.rs:203] File: README.md found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/README.md"
INFO 2025-12-07 23:48:08.379 public.rs:52] Skipping download, using cached copy of file: README.md at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/README.md"
INFO 2025-12-07 23:48:08.385 lib.rs:203] File: special_tokens_map.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/special_tokens_map.json"
INFO 2025-12-07 23:48:08.385 public.rs:52] Skipping download, using cached copy of file: special_tokens_map.json at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/special_tokens_map.json"
INFO 2025-12-07 23:48:08.391 lib.rs:203] File: generation_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/generation_config.json"
INFO 2025-12-07 23:48:08.391 public.rs:52] Skipping download, using cached copy of file: generation_config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/generation_config.json"
INFO 2025-12-07 23:48:08.396 lib.rs:203] File: checksums.blake3 found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/checksums.blake3"
INFO 2025-12-07 23:48:08.397 public.rs:52] Skipping download, using cached copy of file: checksums.blake3 at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/checksums.blake3"
INFO 2025-12-07 23:48:08.402 lib.rs:203] File: config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/config.json"
INFO 2025-12-07 23:48:08.402 public.rs:52] Skipping download, using cached copy of file: config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/config.json"
INFO 2025-12-07 23:48:08.408 lib.rs:203] File: tokenizer.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/tokenizer.json"
INFO 2025-12-07 23:48:08.408 public.rs:52] Skipping download, using cached copy of file: tokenizer.json at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/tokenizer.json"
INFO 2025-12-07 23:48:08.414 lib.rs:203] File: tokenizer_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/tokenizer_config.json"
INFO 2025-12-07 23:48:08.414 public.rs:52] Skipping download, using cached copy of file: tokenizer_config.json at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/tokenizer_config.json"
INFO 2025-12-07 23:48:08.420 lib.rs:203] File: LICENSE.pdf found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/LICENSE.pdf"
INFO 2025-12-07 23:48:08.420 public.rs:52] Skipping download, using cached copy of file: LICENSE.pdf at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/LICENSE.pdf"
INFO 2025-12-07 23:48:08.426 lib.rs:203] File: model-00001-of-00002.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/model-00001-of-00002.safetensors"
INFO 2025-12-07 23:48:08.426 public.rs:52] Skipping download, using cached copy of file: model-00001-of-00002.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/model-00001-of-00002.safetensors"
INFO 2025-12-07 23:48:08.432 lib.rs:203] File: model-00002-of-00002.safetensors found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/model-00002-of-00002.safetensors"
INFO 2025-12-07 23:48:08.432 public.rs:52] Skipping download, using cached copy of file: model-00002-of-00002.safetensors at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/model-00002-of-00002.safetensors"
INFO 2025-12-07 23:48:08.438 lib.rs:203] File: model.safetensors.index.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/model.safetensors.index.json"
INFO 2025-12-07 23:48:08.438 public.rs:52] Skipping download, using cached copy of file: model.safetensors.index.json at path: "/opt/nim/.cache/ngc/hub/models--nim--meta--llama-3.1-8b-instruct-dgx-spark/snapshots/hf-fp8-42d9515+chk1/model.safetensors.index.json"
INFO 2025-12-07 23:48:08.439 nim_sdk.py:346] Using the workspace specified during init: /opt/nim/workspace
INFO 2025-12-07 23:48:08.439 nim_sdk.py:352] Creating workspace at /opt/nim/workspace
INFO 2025-12-07 23:48:08.439 nim_sdk.py:359] Materializing workspace to: /opt/nim/workspace
INFO 2025-12-07 23:48:09.523 nimutils.py:259] Starting NIM inference server
INFO 2025-12-07 23:48:09.523 inference.py:425] Starting application
WARN 2025-12-07 23:48:09.524 system.rs:194] Not supported error accessing attributes of Device with index: 0 error: the requested operation is not available on the target device
INFO 2025-12-07 23:48:09.529 telemetry_handler.py:80] Telemetry mode TelemetryMode.OFF
INFO 2025-12-07 23:48:09.570 inference.py:138] Interface initialized
INFO 2025-12-07 23:48:09.570 inference.py:430] Starting server
INFO 2025-12-07 23:48:09.570 http_api.py:110] Serving endpoints:
  0.0.0.0:8000/v1/models (GET)
  0.0.0.0:8000/v1/chat/completions (POST)
  0.0.0.0:8000/v1/completions (POST)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/version (GET)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/metrics (GET)
  0.0.0.0:8000/v1/license (GET)
  0.0.0.0:8000/v1/metadata (GET)
  0.0.0.0:8000/v1/manifest (GET)
INFO 2025-12-07 23:48:09.571 http_api.py:134] {'message': 'Starting HTTP Inference server', 'port': 8000, 'workers_count': 1, 'host': '0.0.0.0', 'log_level': 'info', 'SSL': 'disabled'}
INFO 2025-12-07 23:48:09.604 server.py:82] Started server process [79]
INFO 2025-12-07 23:48:09.605 on.py:48] Waiting for application startup.
INFO 2025-12-07 23:48:09.605 inference.py:144] Starting engine initialization with model: meta/llama-3.1-8b-instruct
INFO 2025-12-07 23:48:09.671 vllm_backend.py:71] Initializing tokenizer
INFO 2025-12-07 23:48:09.671 vllm_backend.py:78] Using tokenizer source: /opt/nim/workspace
INFO 2025-12-07 23:48:09.888 vllm_backend.py:84] Tokenizer initialized successfully
INFO 2025-12-07 23:48:09.888 vllm_backend.py:106] Using model source for engine: /opt/nim/workspace
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-07 23:48:13 [__init__.py:711] Resolved architecture: LlamaForCausalLM
INFO 12-07 23:48:13 [__init__.py:1750] Using max model len 8192
WARNING 12-07 23:48:14 [arg_utils.py:1775] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.
INFO 12-07 23:48:14 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 12-07 23:48:14 [modelopt.py:71] Detected ModelOpt fp8 checkpoint. Please note that the format is experimental and could change.
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:48:14 [core.py:636] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:48:14 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1+381074ae.nv25.09) with config: model='/opt/nim/workspace', speculative_config=None, tokenizer='/opt/nim/workspace', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/nim/workspace, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":8,"local_cache_dir":null}
[W1207 23:48:15.288406629 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:48:15 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:48:15 [topk_topp_sampler.py:50] Using FlashInfer for top-p & top-k sampling.
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:48:15 [gpu_model_runner.py:1953] Starting to load model /opt/nim/workspace...
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:48:15 [gpu_model_runner.py:1985] Loading model from scratch...
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:48:16 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=194)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_0 pid=194)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:26<00:26, 26.39s/it]
[1;36m(EngineCore_0 pid=194)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 28.32s/it]
[1;36m(EngineCore_0 pid=194)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 28.03s/it]
[1;36m(EngineCore_0 pid=194)[0;0m 
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:12 [default_loader.py:262] Loading weights took 56.42 seconds
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:13 [gpu_model_runner.py:2007] Model loading took 8.4890 GiB and 57.278174 seconds
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:17 [backends.py:548] Using cache directory: /tmp/vllm/cache/torch_compile_cache/6dbf84b8ba/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:17 [backends.py:559] Dynamo bytecode transform time: 4.05 s
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:19 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:24 [backends.py:215] Compiling a graph for dynamic shape takes 5.96 s
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:26 [monitor.py:34] torch.compile takes 10.01 s in total
[1;36m(EngineCore_0 pid=194)[0;0m INFO 12-07 23:49:52 [gpu_worker.py:276] Available KV cache memory: -5.32 GiB
[1;36m(EngineCore_0 pid=194)[0;0m Process EngineCore_0:
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700] EngineCore failed to start.
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700] Traceback (most recent call last):
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=194)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_0 pid=194)[0;0m     self.run()
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_0 pid=194)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 704, in run_engine_core
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 492, in __init__
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 89, in __init__
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 189, in _initialize_kv_caches
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1095, in get_kv_cache_config
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 682, in check_enough_kv_cache_memory
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700]     raise ValueError("No available memory for the cache blocks. "
[1;36m(EngineCore_0 pid=194)[0;0m ERROR 12-07 23:49:52 [core.py:700] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
[1;36m(EngineCore_0 pid=194)[0;0m     raise e
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[1;36m(EngineCore_0 pid=194)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=194)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 492, in __init__
[1;36m(EngineCore_0 pid=194)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 89, in __init__
[1;36m(EngineCore_0 pid=194)[0;0m     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 189, in _initialize_kv_caches
[1;36m(EngineCore_0 pid=194)[0;0m     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1095, in get_kv_cache_config
[1;36m(EngineCore_0 pid=194)[0;0m     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_0 pid=194)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 682, in check_enough_kv_cache_memory
[1;36m(EngineCore_0 pid=194)[0;0m     raise ValueError("No available memory for the cache blocks. "
[1;36m(EngineCore_0 pid=194)[0;0m ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
ERROR 2025-12-07 23:49:52.814 vllm_backend.py:239] Initialization error: Engine core initialization failed. See root cause above. Failed core proc(s): {}
ERROR 2025-12-07 23:49:52.814 vllm_backend.py:240] Error type: <class 'RuntimeError'>
ERROR 2025-12-07 23:49:52.817 vllm_backend.py:243] Full traceback: Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/backends/vllm_backend.py", line 141, in initialize
    self.engine = AsyncLLMEngine.from_engine_args(engine_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py", line 579, in from_engine_args
    return async_engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 1557, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 174, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 120, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 767, in __init__
    super().__init__(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 446, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 706, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR 2025-12-07 23:49:52.817 inference.py:160] Error during engine lifecycle: Engine core initialization failed. See root cause above. Failed core proc(s): {}
ERROR 2025-12-07 23:49:52.818 on.py:121] Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/starlette/routing.py", line 694, in lifespan
    async with self.lifespan_context(app) as maybe_state:
  File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/nimlib/nim_inference_api_builder/api.py", line 100, in wrapper
    async with lifespan_func():
  File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/nim/inference.py", line 146, in lifespan
    await self.engine.initialize()  # Properly await the async initialization
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/inference_engine.py", line 87, in initialize
    await self.backend.initialize()
  File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/backends/vllm_backend.py", line 141, in initialize
    self.engine = AsyncLLMEngine.from_engine_args(engine_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py", line 579, in from_engine_args
    return async_engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 1557, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 174, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 120, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 767, in __init__
    super().__init__(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 446, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 706, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR 2025-12-07 23:49:52.819 on.py:59] Application startup failed. Exiting.
INFO 2025-12-07 23:49:52.820 http_api.py:170] HTTP Inference server has exited.

=== JOURNALCTL (Last 100 lines) ===
Dec 07 18:50:13 jowestdgx bash[5518]: ERROR 2025-12-07 23:50:13.867 inference.py:165] Error during engine lifecycle: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Dec 07 18:50:13 jowestdgx bash[5518]: ERROR 2025-12-07 23:50:13.868 on.py:121] Traceback (most recent call last):
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/starlette/routing.py", line 694, in lifespan
Dec 07 18:50:13 jowestdgx bash[5518]:     async with self.lifespan_context(app) as maybe_state:
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
Dec 07 18:50:13 jowestdgx bash[5518]:     return await anext(self.gen)
Dec 07 18:50:13 jowestdgx bash[5518]:            ^^^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/nimlib/nim_inference_api_builder/api.py", line 100, in wrapper
Dec 07 18:50:13 jowestdgx bash[5518]:     async with lifespan_func():
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
Dec 07 18:50:13 jowestdgx bash[5518]:     return await anext(self.gen)
Dec 07 18:50:13 jowestdgx bash[5518]:            ^^^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/opt/nim/inference.py", line 151, in lifespan
Dec 07 18:50:13 jowestdgx bash[5518]:     await self.engine.initialize()  # Properly await the async initialization
Dec 07 18:50:13 jowestdgx bash[5518]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/inference_engine.py", line 87, in initialize
Dec 07 18:50:13 jowestdgx bash[5518]:     await self.backend.initialize()
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/nim_inference_sdk/backends/vllm_backend.py", line 144, in initialize
Dec 07 18:50:13 jowestdgx bash[5518]:     self.engine = AsyncLLMEngine.from_engine_args(engine_args)
Dec 07 18:50:13 jowestdgx bash[5518]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py", line 574, in from_engine_args
Dec 07 18:50:13 jowestdgx bash[5518]:     return async_engine_cls.from_vllm_config(
Dec 07 18:50:13 jowestdgx bash[5518]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 1589, in inner
Dec 07 18:50:13 jowestdgx bash[5518]:     return fn(*args, **kwargs)
Dec 07 18:50:13 jowestdgx bash[5518]:            ^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 212, in from_vllm_config
Dec 07 18:50:13 jowestdgx bash[5518]:     return cls(
Dec 07 18:50:13 jowestdgx bash[5518]:            ^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 136, in __init__
Dec 07 18:50:13 jowestdgx bash[5518]:     self.engine_core = EngineCoreClient.make_async_mp_client(
Dec 07 18:50:13 jowestdgx bash[5518]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
Dec 07 18:50:13 jowestdgx bash[5518]:     return AsyncMPClient(*client_args)
Dec 07 18:50:13 jowestdgx bash[5518]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 769, in __init__
Dec 07 18:50:13 jowestdgx bash[5518]:     super().__init__(
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 448, in __init__
Dec 07 18:50:13 jowestdgx bash[5518]:     with launch_core_engines(vllm_config, executor_class,
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
Dec 07 18:50:13 jowestdgx bash[5518]:     next(self.gen)
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
Dec 07 18:50:13 jowestdgx bash[5518]:     wait_for_engine_startup(
Dec 07 18:50:13 jowestdgx bash[5518]:   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
Dec 07 18:50:13 jowestdgx bash[5518]:     raise RuntimeError("Engine core initialization failed. "
Dec 07 18:50:13 jowestdgx bash[5518]: RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Dec 07 18:50:13 jowestdgx bash[5518]: ERROR 2025-12-07 23:50:13.869 on.py:59] Application startup failed. Exiting.
Dec 07 18:50:13 jowestdgx bash[5518]: INFO 2025-12-07 23:50:13.869 http_api.py:171] HTTP Inference server has exited.
Dec 07 18:50:24 jowestdgx bash[4965]: (EngineCore_0 pid=196) INFO 12-07 23:50:24 [gpu_worker.py:276] Available KV cache memory: 31.01 GiB
Dec 07 18:50:24 jowestdgx bash[4965]: (EngineCore_0 pid=196) INFO 12-07 23:50:24 [kv_cache_utils.py:849] GPU KV cache size: 127,008 tokens
Dec 07 18:50:24 jowestdgx bash[4965]: (EngineCore_0 pid=196) INFO 12-07 23:50:24 [kv_cache_utils.py:853] Maximum concurrency for 8,192 tokens per request: 15.50x
Dec 07 18:50:27 jowestdgx bash[4965]: [705B blob data]
Dec 07 18:50:27 jowestdgx bash[4965]: (EngineCore_0 pid=196) INFO 12-07 23:50:27 [gpu_model_runner.py:2708] Graph capturing finished in 1 secs, took 0.09 GiB
Dec 07 18:50:27 jowestdgx bash[4965]: (EngineCore_0 pid=196) INFO 12-07 23:50:27 [core.py:214] init engine (profile, create kv cache, warmup model) took 87.08 seconds
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 12-07 23:50:27 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 7938
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.691 vllm_backend.py:142] Initialized AsyncLLMEngine from /opt/nim/workspace
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.691 vllm_backend.py:145] Initializing model handler
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.692 vllm_backend.py:167] No model handler created for Qwen/Qwen3-32B
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.692 vllm_backend.py:258] No chat template provided, attempting to use model's default template
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.692 vllm_backend.py:270] Attempting to get official chat template from model
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.692 vllm_backend.py:283] Successfully loaded chat template from model
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.692 vllm_backend.py:188] Starting warmup generation
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.835 vllm_backend.py:218] The server is up and ready to serve!
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.835 inference.py:159] Engine initialization complete
Dec 07 18:50:27 jowestdgx bash[4965]: WARNING 12-07 23:50:27 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 12-07 23:50:27 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 12-07 23:50:27 [serving_chat.py:94] "auto" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 12-07 23:50:27 [serving_chat.py:134] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 12-07 23:50:27 [serving_responses.py:120] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 12-07 23:50:27 [serving_responses.py:149] "auto" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 vllm_backend.py:782] Responses serving initialized for VLLM
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 vllm_backend.py:807] OpenAI serving implementation initialized for NIMType.LLM model
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 vllm_backend.py:811] Tool calling enabled with parser: hermes
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 inference.py:163] Serving implementation initialized
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 inference.py:164] curl -X 'POST' \
Dec 07 18:50:27 jowestdgx bash[4965]:   'http://0.0.0.0:8000/v1/chat/completions' \
Dec 07 18:50:27 jowestdgx bash[4965]:   -H 'accept: application/json' \
Dec 07 18:50:27 jowestdgx bash[4965]:   -H 'Content-Type: application/json' \
Dec 07 18:50:27 jowestdgx bash[4965]:   -d '{
Dec 07 18:50:27 jowestdgx bash[4965]:     "model": "Qwen/Qwen3-32B",
Dec 07 18:50:27 jowestdgx bash[4965]:     "messages": [
Dec 07 18:50:27 jowestdgx bash[4965]:       {
Dec 07 18:50:27 jowestdgx bash[4965]:         "role":"system",
Dec 07 18:50:27 jowestdgx bash[4965]:         "content":"detailed thinking on"
Dec 07 18:50:27 jowestdgx bash[4965]:       },
Dec 07 18:50:27 jowestdgx bash[4965]:       {
Dec 07 18:50:27 jowestdgx bash[4965]:         "role":"user",
Dec 07 18:50:27 jowestdgx bash[4965]:         "content":"Can you write me a song?"
Dec 07 18:50:27 jowestdgx bash[4965]:       }
Dec 07 18:50:27 jowestdgx bash[4965]:     ],
Dec 07 18:50:27 jowestdgx bash[4965]:     "top_p": 1,
Dec 07 18:50:27 jowestdgx bash[4965]:     "n": 1,
Dec 07 18:50:27 jowestdgx bash[4965]:     "max_tokens": 15,
Dec 07 18:50:27 jowestdgx bash[4965]:     "frequency_penalty": 1.0,
Dec 07 18:50:27 jowestdgx bash[4965]:     "stop": ["hello"]
Dec 07 18:50:27 jowestdgx bash[4965]:   }'
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 on.py:62] Application startup complete.
Dec 07 18:50:27 jowestdgx bash[4965]: INFO 2025-12-07 23:50:27.902 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
Dec 07 18:50:41 jowestdgx bash[4965]: INFO 2025-12-07 23:50:41.017 httptools_impl.py:481] 192.168.88.247:63149 - "GET /v1/models HTTP/1.1" 200
Dec 07 18:50:41 jowestdgx bash[4965]: INFO 2025-12-07 23:50:41.556 httptools_impl.py:481] 192.168.88.247:63149 - "GET /v1/models HTTP/1.1" 200

=== CONFIG FILES ===
/etc/nvidia-nim/qwen32b.env:IMAGE_NAME=nvcr.io/nim/qwen/qwen3-32b-dgx-spark:1.1.0-variant
/etc/nvidia-nim/qwen32b.env:PORT=8001
/etc/nvidia-nim/qwen32b.env:GPU_MEM_FRACTION=0.50
/etc/nvidia-nim/qwen32b.env:NGC_API_KEY=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
/etc/nvidia-nim/qwen32b.env:SHM_SIZE=16g
/etc/nvidia-nim/nemotron9b.env:IMAGE_NAME=nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2-dgx-spark:1.0.0-variant
/etc/nvidia-nim/nemotron9b.env:PORT=8003
/etc/nvidia-nim/nemotron9b.env:GPU_MEM_FRACTION=0.15
/etc/nvidia-nim/nemotron9b.env:NGC_API_KEY=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
/etc/nvidia-nim/nemotron9b.env:SHM_SIZE=16g
/etc/nvidia-nim/nemotron9b.env:NIM_CONFIG_FILE=/etc/nvidia-nim/nemotron9b-config.yaml
/etc/nvidia-nim/llama8b-config.yaml:context_length: 32768      # you probably donâ€™t need 128k for coding
/etc/nvidia-nim/llama8b-config.yaml:mem_fraction_static: 0.65  # less KV cache vs default 0.8-ish
/etc/nvidia-nim/nemotron9b-config.yaml:context_length: 32768      # you probably donâ€™t need 128k for coding
/etc/nvidia-nim/nemotron9b-config.yaml:mem_fraction_static: 0.65  # less KV cache vs default 0.8-ish
/etc/nvidia-nim/llama8b.env:IMAGE_NAME=nvcr.io/nim/meta/llama-3.1-8b-instruct-dgx-spark:1.0.0-variant
/etc/nvidia-nim/llama8b.env:PORT=8002
/etc/nvidia-nim/llama8b.env:GPU_MEM_FRACTION=0.15
/etc/nvidia-nim/llama8b.env:NGC_API_KEY=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
/etc/nvidia-nim/llama8b.env:SHM_SIZE=16g
/etc/nvidia-nim/llama8b.env:NIM_CONFIG_FILE=/etc/nvidia-nim/llama8b-config.yaml

