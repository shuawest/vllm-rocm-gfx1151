# Track E: The Vulkan Speedster
# Goal: Build llama.cpp with Vulkan backend for max prompt processing speed on gfx1151.

FROM ubuntu:24.04

# Install build dependencies and Vulkan SDK (System packages are new enough in 24.04)
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    wget \
    gnupg \
    && wget -qO - https://packages.lunarg.com/lunarg-signing-key-pub.asc | apt-key add - \
    && wget -qO /etc/apt/sources.list.d/lunarg-vulkan-noble.list https://packages.lunarg.com/vulkan/lunarg-vulkan-noble.list \
    && apt-get update && apt-get install -y \
    vulkan-sdk \
    libcurl4-openssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Clone llama.cpp
WORKDIR /app
RUN git clone https://github.com/ggerganov/llama.cpp.git

# Build with Vulkan support
WORKDIR /app/llama.cpp
RUN cmake -B build -DGGML_VULKAN=ON && \
    cmake --build build --config Release -j$(nproc)

# Expose server port
EXPOSE 8080

# Default entrypoint
ENTRYPOINT ["./build/bin/llama-server"]
CMD ["--help"]
