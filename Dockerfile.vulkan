# Track E: The Vulkan Speedster
# Goal: Build llama.cpp with Vulkan backend for max prompt processing speed on gfx1151.

FROM ubuntu:22.04

# Install build dependencies and Vulkan SDK
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    wget \
    libvulkan-dev \
    vulkan-tools \
    && rm -rf /var/lib/apt/lists/*

# Clone llama.cpp
WORKDIR /app
RUN git clone https://github.com/ggerganov/llama.cpp.git

# Build with Vulkan support
WORKDIR /app/llama.cpp
RUN cmake -B build -DGGML_VULKAN=ON && \
    cmake --build build --config Release -j$(nproc)

# Expose server port
EXPOSE 8080

# Default entrypoint
ENTRYPOINT ["./build/bin/llama-server"]
CMD ["--help"]
