# syntax=docker/dockerfile:1.4
FROM ubuntu:24.04

# ====== Version Configuration (from versions.env) ======
ARG ROCM_VERSION="6.3.4"
ARG TORCH_VERSION="2.5.0+rocm6.3"
ARG PYTHON_VERSION="3.12"
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"
ARG VLLM_VERSION="v0.10.2"  # Latest stable release
ARG FLASH_ATTENTION_REPO="https://github.com/ROCm/flash-attention.git"
ARG FLASH_ATTENTION_COMMIT="main_perf"

LABEL maintainer="Antigravity Build System"
LABEL description="vLLM + PyTorch on ROCm gfx1151 (Strix Point/Halo) - Stable Versions"
LABEL vllm.version="${VLLM_VERSION}"
LABEL torch.version="${TORCH_VERSION}"
LABEL rocm.version="${ROCM_VERSION}"

# ====== Base System Setup ======
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y \
    git \
    wget \
    curl \
    vim \
    rsync \
    dialog \
    software-properties-common \
    ninja-build \
    # Add deadsnakes PPA for Python 3.10
    && add-apt-repository ppa:deadsnakes/ppa -y \
    && apt-get update \
    && apt-get install -y \
    python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python3-pip python${PYTHON_VERSION}-venv \
    libssl-dev libffi-dev \
    ca-certificates tar gzip \
    # Install ROCm (Stable 6.2)
    && wget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/noble/amdgpu-install_6.2.60204-1_all.deb \
    && apt-get install -y ./amdgpu-install_6.2.60204-1_all.deb \
    && rm amdgpu-install_6.2.60204-1_all.deb \
    && amdgpu-install -y --usecase=rocm --no-dkms \
    && rm -rf /var/lib/apt/lists/*

# Set python3 to point to our version
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 1

ENV ROCM_PATH=/opt/rocm
ENV LD_LIBRARY_PATH=$ROCM_PATH/lib:$LD_LIBRARY_PATH
ENV PATH=$ROCM_PATH/bin:$PATH
ENV DEVICE_LIB_PATH=$ROCM_PATH/amdgcn/bitcode
ENV HIP_DEVICE_LIB_PATH=$ROCM_PATH/amdgcn/bitcode
ENV PYTORCH_ROCM_ARCH="gfx1151"
ENV HSA_OVERRIDE_GFX_VERSION=11.5.1

# ====== Install uv for Python package management ======
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /opt/vllm-build
RUN uv venv --python ${PYTHON_VERSION}
ENV VIRTUAL_ENV=/opt/vllm-build/.venv
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# ====== Install PyTorch 2.5.0 (Stable) ======
# Note: Using latest from ROCm 6.2 index (version constraint doesn't work with index-url)
# Install PyTorch from TheRock Nightlies (gfx1151 support)
# We need to trust the repo or use --break-system-packages if needed (but we are in venv)
RUN uv pip install --index-url https://rocm.nightlies.amd.com/v2/gfx1151/torch_nightly/ \
    --extra-index-url https://download.pytorch.org/whl/rocm6.2 \
    torch torchvision torchaudio && \
    # Pin CMake to avoid incompatibility with ROCm 6.2 config files
    uv pip install cmake==3.29.0 && \
    # Verify PyTorch version and Arch
    python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'ROCm: {torch.version.hip}')"

# Verify torch installation
RUN python -c "import torch; print(f'PyTorch: {torch.__version__}, ROCm: {torch.version.hip}')"

# ====== Clone vLLM v0.10.3 (Release Tag) ======
RUN git clone "${VLLM_REPO}" vllm && \
    cd vllm && \
    git fetch --all --tags && \
    git checkout "${VLLM_VERSION}" && \
    # Patch: Add gfx1151 to supported architectures (robust patch)
    sed -i 's/set(HIP_SUPPORTED_ARCHS "/set(HIP_SUPPORTED_ARCHS "gfx1151;/g' CMakeLists.txt && \
    # Patch vLLM platform detection (Issue #20)
    sed -i 's/return "vllm.platforms.rocm.RocmPlatform" if is_rocm else None/import torch; return "vllm.platforms.rocm.RocmPlatform" if (is_rocm or torch.version.hip) else None/g' vllm/platforms/__init__.py && \
    sed -i 's/import amdsmi/try:\n            import amdsmi\n        except ImportError:\n            pass/g' vllm/platforms/__init__.py && \
    sed -i 's/if len(amdsmi.amdsmi_get_processor_handles()) > 0:/if torch.version.hip:\n                is_rocm = True\n            elif len(amdsmi.amdsmi_get_processor_handles()) > 0:/g' vllm/platforms/__init__.py && \
    # Patch vLLM fp8_utils.py for PyTorch 2.5.1 compatibility (Issue #25)
    sed -i 's/from typing import Any, Callable, Optional, Union/from typing import Any, Callable, Optional, Union, List/g' vllm/model_executor/layers/quantization/utils/fp8_utils.py && \
    sed -i 's/block_size: list\[int\]/block_size: List[int]/g' vllm/model_executor/layers/quantization/utils/fp8_utils.py

WORKDIR /opt/vllm-build/vllm

# ====== Build vLLM ======
ENV VLLM_TARGET_DEVICE="rocm"
ENV MAX_JOBS="4"
ENV FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE"

# Install vLLM dependencies
RUN uv pip install -r requirements/rocm.txt && \
    uv pip install \
    setuptools setuptools_scm wheel ninja \
    cmake scikit-build-core

# Build and install vLLM
RUN CMAKE_PREFIX_PATH="${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch/share/cmake/Torch" \
    TORCH_CUDA_ARCH_LIST="gfx1151" \
    uv pip install --no-build-isolation -e .

# ====== Optional: Build Flash Attention ======
RUN cd /opt/vllm-build && \
    git clone "${FLASH_ATTENTION_REPO}" flash-attention && \
    cd flash-attention && \
    git checkout "${FLASH_ATTENTION_COMMIT}" && \
    MAX_JOBS=4 FLASH_ATTENTION_TRITON_AMD_ENABLE=TRUE \
    python setup.py install

# ====== Verification & Runtime Setup ======
RUN python -c "import vllm; print(f'vLLM: {vllm.__version__}')" && \
    python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Devices: {torch.cuda.device_count()}')"

WORKDIR /workspace
EXPOSE 8000

CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
    "--host", "0.0.0.0", "--port", "8000", \
    "--model", "cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit", \
    "--dtype", "float16", \
    "--max-model-len", "32768"]