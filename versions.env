# Reproducible Build Configuration - Stable Versions
# Updated: 2025-12-03
# Strategy: Use stable ROCm/PyTorch/vLLM instead of bleeding-edge nightlies

# ROCm Configuration (Stable Release)
ROCM_VERSION="6.2.4"
# Alternative: Use AMD's pre-built Docker image
# BASE_IMAGE="rocm/vllm-dev:rocm7.1.1_navi_ubuntu24.04_py3.12_pytorch_2.8_vllm_0.10.2rc1"

# PyTorch Version (Stable)  
TORCH_VERSION="2.5.1+rocm6.2"
# Note: Install from PyTorch official ROCm index, NOT TheRock nightlies

# vLLM Source (Release Tag)
VLLM_REPO="https://github.com/vllm-project/vllm.git"
VLLM_VERSION="v0.10.2"  # Latest stable release tag

# Flash Attention Source (Stable Commit)
FLASH_ATTENTION_REPO="https://github.com/ROCm/flash-attention.git"
FLASH_ATTENTION_COMMIT="main_perf"  # Use branch tip from late 2024

# Build Settings
PYTHON_VERSION="3.12"
IMAGE_NAME="strix-vllm"
IMAGE_TAG="v0.10.3-rocm6.3"
