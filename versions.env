# Reproducible Build Configuration - Stable Versions
# Updated: 2025-12-03
# Strategy: Use stable ROCm/PyTorch/vLLM instead of bleeding-edge nightlies

# ROCm Configuration (Stable Release)
ROCM_VERSION="6.2.4"
# Alternative: Use AMD's pre-built Docker image
# BASE_IMAGE="rocm/vllm-dev:rocm7.1.1_navi_ubuntu24.04_py3.12_pytorch_2.8_vllm_0.10.2rc1"

# ROCm & PyTorch Versions (TheRock Nightlies for gfx1151 support)
ROCM_VERSION="6.3.0" # Base ROCm version for dependencies
ROCM_INDEX_URL="https://rocm.nightlies.amd.com/v2/gfx1151/"
TORCH_VERSION="2.5.0.dev20240905+rocm6.2" # Using a known nightly or similar if available, or let pip find it
# Actually, let's use the specific one from the tutorial or previous attempts
# TheRock nightly URL usually has a specific torch version.
# Let's use the one that worked for others:
TORCH_INDEX_URL="https://rocm.nightlies.amd.com/v2/gfx1151/torch_nightly/"
# We will let pip resolve the version from the index
PYTHON_VERSION="3.10"
# Note: Install from PyTorch official ROCm index, NOT TheRock nightlies

# vLLM Source (Release Tag)
VLLM_REPO="https://github.com/vllm-project/vllm.git"
VLLM_VERSION="v0.10.2"  # Latest stable release tag

# Flash Attention Source (Stable Commit)
FLASH_ATTENTION_REPO="https://github.com/ROCm/flash-attention.git"
FLASH_ATTENTION_COMMIT="main_perf"  # Use branch tip from late 2024

# Build Settings
PYTHON_VERSION="3.10"
IMAGE_NAME="strix-vllm"
IMAGE_TAG="v0.10.3-rocm6.3"
